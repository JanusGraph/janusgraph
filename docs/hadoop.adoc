[[hadoop-tp3]]
== JanusGraph with Tinker Pop's Hadoop-Gremlin

JanusGraph-Hadoop works with Tinker Pop's hadoop-gremlin package for
general-purpose OLAP.

As part of scope of documentation, the following topics will be covered:

* Configuring Hadoop for running OLAP:

  - For executing any Analytical queries by leveraging Apache's Parallel computation framework like Apache Spark or Apache Giraph, Hadoop configuration would be required.
  This is because, Hadoop is a distributed access based File System, and Spark/Giraph workers are spread across multiple machines.
  Hadoop, in this scenario HDFS enables a common share point which all the workers can access to store their intermediate results.

* Bulk Loading Graph into JanusGraph:

  - Apache Tinker Pop provides a framework to bulk insert data into JanusGraph by using Vertex Programs. A Vertex Program is a program which is executed
  at each vertex, and is distributed across all workers nodes. While only Bulk Loading Vertex Program is coverted as scope of this tutorial, but for further docs about
  Vertex Programs, please refer to official http://tinkerpop.apache.org/docs/$MAVEN{tinkerpop.version}/reference/#_a_collection_of_vertexprograms[Tinker Pop VertexProgram docs].

* OLAP Traversals using HadoopGraph:

  - JanusGraph provides Class Implementations to read the Graph persisted in backend data store (Cassandra/HBase etc) into HadoopGraph.
  which can then leverage the power of Spark/Giraph to do OLAP traversals over it in distributed manner.

For the scope of bellow tutorial, the Computing framework selected is Apache Spark, and backend data store is Apache Cassandra.

[NOTE]
The examples in this chapter are based on running Spark in local mode or standalone cluster mode. Additional configuration
is required when using Spark on YARN or Mesos.

=== Configuring Hadoop for running OLAP
For running OLAP queries from The Gremlin Console, a few pre requisites needs to be fulfilled. You will need to add configuration directory of Hadoop into `CLASSPATH`, and the configuration directory needs to point to a live Hadoop Cluster.

Hadoop is Distributed access controlled file system. The Hadoop file system is used by Spark workers running on different machines to have a common source for file based operations. The intermediate computations of various OLAP queries if needed to be persisted are persisted on Hadoop File System.

For configuring Single Node Hadoop Cluster, please refer to official https://hadoop.apache.org/docs/r2.7.6/hadoop-project-dist/hadoop-common/SingleCluster.html[Apache Hadoop Docs]

Once you have Hadoop Cluster up and running, we will need to specify the Hadoop Configuration files in `CLASSPATH`. The bellow document expects that you have those configuration present at `/etc/hadoop/conf`

Once verified, follow the bellow steps to add Hadoop configuration to The Gremlin Console's (Our Spark Driver) `CLASSPATH`.

[source, shell]
----
export HADOOP_CONF_DIR=/etc/hadoop/conf
export CLASSPATH=$HADOOP_CONF_DIR
----

Once Path to Hadoop Configuration has been added to `CLASSPATH`, we can verify weather The Gremlin Console can access our Hadoop cluster or not by following bellow quick steps:

[source, gremlin]
----
gremlin> hdfs
==>storage[org.apache.hadoop.fs.LocalFileSystem@65bb9029] // BAD

gremlin> hdfs
==>storage[DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_1229457199_1, ugi=user (auth:SIMPLE)]]] // GOOD
----

=== Running Bulk Loading Vertex Program (BLVP).
Apache Tinker Pop provides various Vertex Programs. A Vertex Program is a program which runs on each vertex till either an termination criteria is attained, or fixed number of iteration has reached. Due to parallel nature of programs, they can leverage Parallel Computing Frameworks like Spark, Giraph to improve their performances.

The following sections walks us through using Bulk Loading Vertex Program leveraging distributed computation capabilities of Spark.

==== Bulk loading with local Spark
Here's an example showing some basic integrated JanusGraph-Tinker Pop functionality running Spark in local mode. A complete Gremlin Console session is shown below:


[source, gremlin]
----
bin/gremlin.sh

         \,,,/
         (o o)
-----oOOo-(3)-oOOo-----
plugin activated: janusgraph.imports
gremlin> :plugin use tinkerpop.hadoop
==>tinkerpop.hadoop activated
gremlin> :plugin use tinkerpop.spark
==>tinkerpop.spark activated
gremlin> // 1. Load commands from a script file for generating the graph schema
gremlin> :load data/grateful-dead-janusgraph-schema.groovy
==>true
==>true
gremlin> // 2. Open the graph and run the command to load the schema
gremlin> graph = JanusGraphFactory.open('conf/janusgraph-cql.properties')
==>standardjanusgraph[cql:[127.0.0.1]]
gremlin> // 3. Generate the schema, and commit to graph.
gremlin> defineGratefulDeadSchema(graph)
==>null
gremlin> graph.close()
==>null
gremlin>
gremlin> // 4. Copy the input file into HDFS
gremlin> if (!hdfs.exists('/grateful-dead.kryo')) hdfs.copyFromLocal('data/grateful-dead.kryo','/grateful-dead.kryo')
==>null
gremlin>
gremlin> // 5. Open a the graph for OLAP processing. The graph will be read from the file previously uploaded into HDFS.
gremlin> graph = GraphFactory.open('conf/hadoop-graph/hadoop-load.properties')
==>hadoopgraph[gryoinputformat->nulloutputformat]
gremlin> // 6. Configure the vertex program to run over OLAP. The bulk loader program will write into JanusGraph.
gremlin> blvp = BulkLoaderVertexProgram.build().writeGraph('conf/janusgraph-cql.properties').create(graph)
==>BulkLoaderVertexProgram[bulkLoader=IncrementalBulkLoader,vertexIdProperty=bulkLoader.vertex.id,userSuppliedIds=false,keepOriginalIds=true,batchSize=0]
gremlin> graph.compute(SparkGraphComputer).program(blvp).submit().get()
...
==>result[hadoopgraph[gryoinputformat->nulloutputformat],memory[size:0]]
gremlin> graph.close()
==>null
gremlin>
gremlin> 7. Open the JanusGraph instance and verify the data was loaded.
gremlin> graph = JanusGraphFactory.open('conf/janusgraph-cql.properties')
==>standardjanusgraph[cql:[127.0.0.1]]
gremlin> g = graph.traversal()
==>graphtraversalsource[standardjanusgraph[cassandrathrift:[127.0.0.1]], standard]
gremlin> g.V().count()
==>808
gremlin> g.E().count()
==> 8046
----

The following properties is changed from the default `conf/hadoop-graph/hadoop-load.properties` which comes with distribution.
[source, properties]
----
gremlin.hadoop.inputLocation=hdfs:///grateful-dead.kryo
----

[source, properties]
----
# hadoop-load.properties
#
# Hadoop Graph Configuration
#
gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphReader=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoInputFormat
gremlin.hadoop.graphWriter=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat

gremlin.hadoop.jarsInDistributedCache=true
gremlin.hadoop.inputLocation=hdfs:///grateful-dead.kryo
gremlin.hadoop.outputLocation=output
gremlin.spark.persistContext=true

#
# SparkGraphComputer Configuration
#
spark.master=local[*]
spark.executor.memory=1g
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoRegistrator
----

==== Bulk Loading with Spark Standalone Cluster
The steps followed in the previous section can also be used with a Spark standalone cluster with only minor changes:

* Update the spark.master property to point to the Spark master URL instead of local
* Update the spark.executor.extraClassPath to enable the Spark executor to find the JanusGraph dependency jars
* Copy the JanusGraph dependency jars into the location specified in the previous step on each Spark executor machine

[NOTE]
We have copied all the jars under *janusgraph-distribution/lib* into /opt/lib/janusgraph/ and the same directory structure is created across all workers, and jars are manually copied across all workers.

The final properties file used for Hadoop Loading is as follows:

[source, properties]
----
# hadoop-load.properties
#
# Hadoop Graph Configuration
#
gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphReader=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoInputFormat
gremlin.hadoop.graphWriter=org.apache.hadoop.mapreduce.lib.output.NullOutputFormat

gremlin.hadoop.jarsInDistributedCache=true
gremlin.hadoop.inputLocation=hdfs:///grateful-dead.kryo
gremlin.hadoop.outputLocation=output
gremlin.spark.persistContext=true

#
# SparkGraphComputer Configuration
#
spark.master=spark://127.0.0.1:7077
spark.executor.memory=1g
spark.executor.extraClassPath=/opt/lib/janusgraph/*
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoRegistrator
----

=== OLAP Traversals

JanusGraph-Hadoop works with Tinker Pop's hadoop-gremlin package for general-purpose OLAP to traverse over the graph, and parallelize queries by leveraging Apache Spark.

==== OLAP Traversals with Spark Local

The backend of choice here is Cassandra for all OLAP example bellow. Additional configuration will be needed specific to that data store. The configuration is specified by `gremlin.hadoop.graphReader` property which specifies the Class to read data from underlaying backend data store.

JanusGraph currently supports following graphReader classes:

* `Cassandra3InputFormat` for use with Cassandra 3
* `CassandraInputFormat` for use with Cassandra 2
* `HBaseInputFormat` for use with HBase.

The following properties file can be used to connect a JanusGraph instance in Cassandra such that it can be used with HadoopGraph to run OLAP queries.

[source, properties]
----
# read-cassandra3.properties
#
# Hadoop Graph Configuration
#
gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.cassandra.Cassandra3InputFormat
gremlin.hadoop.graphWriter=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat

gremlin.hadoop.jarsInDistributedCache=true
gremlin.hadoop.inputLocation=none
gremlin.hadoop.outputLocation=output
gremlin.spark.persistContext=true

#
# JanusGraph Cassandra InputFormat configuration
#
# These properties defines the connection properties which were used while write data to JanusGraph.
janusgraphmr.ioformat.conf.storage.backend=cassandra
# This specifies the hostname & port for Cassandra data store.
janusgraphmr.ioformat.conf.storage.hostname=127.0.0.1
janusgraphmr.ioformat.conf.storage.port=9160
# This specifies the keyspace where data is stored. In case of HBase, this is referenced as table.
janusgraphmr.ioformat.conf.storage.cassandra.keyspace=janusgraph
# This defines the indexing backned configuration used while writing data to JanusGraph.
janusgraphmr.ioformat.conf.index.search.backend=elasticsearch
janusgraphmr.ioformat.conf.index.search.hostname=127.0.0.1
# Additional property will be required if indexing backend is either Solr or if backend data store is HBase.

#
# Apache Cassandra InputFormat configuration
#
cassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner

#
# SparkGraphComputer Configuration
#
spark.master=local[*]
spark.executor.memory=1g
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoRegistrator

----

First create a properties file with above configurations, and load the same on the Gremlin Console to run OLAP queries as follows:

[source, gremlin]
----
bin/gremlin.sh

         \,,,/
         (o o)
-----oOOo-(3)-oOOo-----
plugin activated: janusgraph.imports
gremlin> :plugin use tinkerpop.hadoop
==>tinkerpop.hadoop activated
gremlin> :plugin use tinkerpop.spark
==>tinkerpop.spark activated
gremlin> graph = GraphFactory.open('conf/hadoop-graph/hadoop-cassandra3.properties')
==>hadoopgraph[cassandra3inputformat->gryooutputformat]
gremlin> g = graph.traversal().withComputer(SparkGraphComputer)
==>graphtraversalsource[hadoopgraph[cassandra3inputformat->gryooutputformat], sparkgraphcomputer]
gremlin> g.V().count()
......
==>808
gremlin> g.E().count()
......
==> 8046
----

==== OLAP Traversals with Spark Standalone Cluster

The steps followed in the previous section can also be used with a Spark standalone cluster with only minor changes:

* Update the spark.master property to point to the Spark master URL instead of local
* Update the spark.executor.extraClassPath to enable the Spark executor to find the JanusGraph dependency jars
* Copy the JanusGraph dependency jars into the location specified in the previous step on each Spark executor machine

[NOTE]
We have copied all the jars under *janusgraph-distribution/lib* into /opt/lib/janusgraph/ and the same directory structure is created across all workers, and jars are manually copied across all workers.

The final properties file used for OLAP traversal is as follows:

[source, properties]
----
# read-cassandra3.properties
#
# Hadoop Graph Configuration
#
gremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph
gremlin.hadoop.graphReader=org.janusgraph.hadoop.formats.cassandra.Cassandra3InputFormat
gremlin.hadoop.graphWriter=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat

gremlin.hadoop.jarsInDistributedCache=true
gremlin.hadoop.inputLocation=none
gremlin.hadoop.outputLocation=output
gremlin.spark.persistContext=true

#
# JanusGraph Cassandra InputFormat configuration
#
# These properties defines the connection properties which were used while write data to JanusGraph.
janusgraphmr.ioformat.conf.storage.backend=cassandra
# This specifies the hostname & port for Cassandra data store.
janusgraphmr.ioformat.conf.storage.hostname=127.0.0.1
janusgraphmr.ioformat.conf.storage.port=9160
# This specifies the keyspace where data is stored. In case of HBase, this is referenced as table.
janusgraphmr.ioformat.conf.storage.cassandra.keyspace=janusgraph
# This defines the indexing backned configuration used while writing data to JanusGraph.
janusgraphmr.ioformat.conf.index.search.backend=elasticsearch
janusgraphmr.ioformat.conf.index.search.hostname=127.0.0.1
# Additional property will be required if indexing backend is either Solr or if backend data store is HBase.

#
# Apache Cassandra InputFormat configuration
#
cassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner

#
# SparkGraphComputer Configuration
#
spark.master=spark://127.0.0.1:7077
spark.executor.memory=1g
spark.executor.extraClassPath=/opt/lib/janusgraph/*
spark.serializer=org.apache.spark.serializer.KryoSerializer
spark.kryo.registrator=org.apache.tinkerpop.gremlin.spark.structure.io.gryo.GryoRegistrator
----

Then use the properties file as follows from the Gremlin Console:

[source, gremlin]
----
bin/gremlin.sh

         \,,,/
         (o o)
-----oOOo-(3)-oOOo-----
plugin activated: janusgraph.imports
gremlin> :plugin use tinkerpop.hadoop
==>tinkerpop.hadoop activated
gremlin> :plugin use tinkerpop.spark
==>tinkerpop.spark activated
gremlin> graph = GraphFactory.open('conf/hadoop-graph/spark-cassandra3.properties')
==>hadoopgraph[cassandra3inputformat->gryooutputformat]
gremlin> g = graph.traversal().withComputer(SparkGraphComputer)
==>graphtraversalsource[hadoopgraph[cassandra3inputformat->gryooutputformat], sparkgraphcomputer]
gremlin> g.V().count()
......
==>808
gremlin> g.E().count()
......
==> 8046
----


=== Other Vertex Programs

Once you are familiar with how to configure JanusGraph to work with Spark, you can run all the other vertex programs provided by Apache TinkerPop, like Page Rank and Peer Pressure. See the http://tinkerpop.apache.org/docs/$MAVEN{tinkerpop.version}/reference/#vertexprogram[TinkerPop VertexProgram docs] for more details.
